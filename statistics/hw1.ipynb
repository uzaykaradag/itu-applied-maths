{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework #1 | MAT244E - Statistics\n",
    "**Submission by: Uzay Karadağ | 090200738**\n",
    "\n",
    "**Lecturer: Asst. Prof. Gül İnan**\n",
    "\n",
    "\n",
    "**Starting off**\n",
    "\n",
    "We start off by importing the modules we need in order the perform to computations, as specified in the homework I used *JAX* and to further learn the grits of the library I decided to go all *JAX* instead of mixing it with *numpy* and *scipy*.\n",
    "\n",
    "Modules:\n",
    "1. *jax.numpy* : numpy implementation from the creators of JAX,\n",
    "2. *jax.random* : used for the generation of dataset, trivial use case,\n",
    "3. *jax.scipy.optimize.minimize* : the most important method here, used for the optimization of the MLE in this case the minimization of the negative log-likelihood function,\n",
    "4. *jax.scipy.stats.norm* : used in the nll (negative log-likelihood) function i built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax.scipy.optimize import minimize\n",
    "from jax.scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The random normal distributed sample generator: *gen_sample***\n",
    "\n",
    "In the next cell I construct a function which takes one parameter:\n",
    "*theta* : a jax.numpy array consisting of 2 elements and 1 dimension, first element is the mean and the second element is the sigma.\n",
    "\n",
    "The output is a randomly generated normally distributed sample of size **1000**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample(theta):\n",
    "    mu = theta[0]\n",
    "    sig = theta[1]\n",
    "    key = random.PRNGKey(738)\n",
    "    sample = random.normal(key, shape=(1000,)) * sig + mu\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actually generating the sample for the problem**\n",
    "\n",
    "Next, I generate the sample we are going to use for the problem using the *gen_sample* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_act = jnp.array([0, 10])\n",
    "x = gen_sample(theta_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the negative log-likelihood function**\n",
    "\n",
    "I defined the negative log-likelihood function such that it takes only one parameter like *gen_sample*:\n",
    "*theta* : same definition we used above applies here as well\n",
    "\n",
    "I used the *jnp.sum* and *norm.logpdf* functions I imported for readability and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(theta):\n",
    "    mu = theta[0]\n",
    "    sig = theta[1]\n",
    "    return -jnp.sum(norm.logpdf(x, loc=mu, scale=sig))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimization of the negative log-likelihood function**\n",
    "\n",
    "We start off by creating a *jax.numpy* array of the same shape as the actual theta vector we used to construct the random data, *jax.numpy.ones* method is used here to do that job. \n",
    "\n",
    "Then we use the *jax.scipy.optimize.minimize* function with three parameters passed to it:\n",
    "1. *fun*: nll, our negative log-likelihood function defined by me\n",
    "2. *x0*: theta_ig, the initial guess for theta hence the theta_ig : Theta **I**nitial **G**uess\n",
    "3. *method*: 'BFGS', this is the only method available for this function as documented in the official documentation of *JAX*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for mu:  0.21785168\n",
      "MLE for sigma:  9.941802\n"
     ]
    }
   ],
   "source": [
    "theta_ig = jnp.ones(theta_act.shape[0])\n",
    "theta_fe = minimize(nll, theta_ig, method='BFGS')\n",
    "# theta_fe.x[0]\n",
    "print(\"MLE for mu: \", theta_fe.x[0])\n",
    "print('MLE for sigma: ', theta_fe.x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "As it can be seen we solved the MLE optimization problem using the JAX library and we estimated mu and sigma semi-accurately."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
