{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from extract import extract_covid_data\n",
    "from transformation import generate_covid_info\n",
    "from load import load_data\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from pyspark.sql.functions import map_concat, lit, create_map, explode, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_utils():\n",
    "    \"\"\"\n",
    "    !!!DO NOT TOUCH!!!\n",
    "    This function returns spark context object and spark session object.\n",
    "    These are the entry point into all functionality in Spark.\n",
    "    :return: SparkContext, SparkSession\n",
    "    \"\"\"\n",
    "    conf = SparkConf().setAppName(\"Covid\"). \\\n",
    "        set(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1\"). \\\n",
    "        set(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1\"). \\\n",
    "        set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\"). \\\n",
    "        set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "    spark = SparkSession.builder.master(\"local[*]\").config(conf=conf).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return sc, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/26 20:28:38 WARN Utils: Your hostname, Uzays-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 161.9.123.126 instead (on interface en0)\n",
      "22/12/26 20:28:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/uzay/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/uzay/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b725e627-bee3-41d4-b8db-510dcc21c532;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 204ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b725e627-bee3-41d4-b8db-510dcc21c532\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/12ms)\n",
      "22/12/26 20:28:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc, spark = get_spark_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = extract_covid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df = spark.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_info = generate_covid_info(raw_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(covid_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df_exploded =  raw_data_df.select(raw_data_df.location, explode(raw_data_df.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df_exploded = raw_data_df_exploded.withColumn(\"location_map\", create_map(\n",
    "        lit(\"location\"), col(\"location\"))).drop(\"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/26 20:29:46 WARN TaskSetManager: Stage 0 contains a task of very large size (17954 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 col|        location_map|\n",
      "+--------------------+--------------------+\n",
      "|{date -> 2020-02-...|{location -> Afgh...|\n",
      "|{date -> 2020-02-...|{location -> Afgh...|\n",
      "|{date -> 2020-02-...|{location -> Afgh...|\n",
      "|{date -> 2020-02-...|{location -> Afgh...|\n",
      "|{date -> 2020-02-...|{location -> Afgh...|\n",
      "|{date -> 2020-02-...|{location -> Afgh...|\n",
      "|{date -> 2020-03-...|{location -> Afgh...|\n",
      "|{date -> 2020-03-...|{location -> Afgh...|\n",
      "|{date -> 2020-03-...|{location -> Afgh...|\n",
      "|{date -> 2020-03-...|{location -> Afgh...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_data_df_exploded.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = raw_data_df_exploded.select(map_concat(\"col\", \"location_map\").alias(\"data_map\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/26 20:29:51 WARN TaskSetManager: Stage 1 contains a task of very large size (17954 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            data_map|\n",
      "+--------------------+\n",
      "|{date -> 2020-02-...|\n",
      "|{date -> 2020-02-...|\n",
      "|{date -> 2020-02-...|\n",
      "|{date -> 2020-02-...|\n",
      "|{date -> 2020-02-...|\n",
      "|{date -> 2020-02-...|\n",
      "|{date -> 2020-03-...|\n",
      "|{date -> 2020-03-...|\n",
      "|{date -> 2020-03-...|\n",
      "|{date -> 2020-03-...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_map.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/26 20:29:54 WARN TaskSetManager: Stage 2 contains a task of very large size (17954 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dml = data_map.select('data_map').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/26 20:30:08 WARN TaskSetManager: Stage 3 contains a task of very large size (15983 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(sc.parallelize(dml)).na.fill('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_info = generate_covid_info(raw_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, excess_mortality: string, excess_mortality_cumulative: string, excess_mortality_cumulative_absolute: string, excess_mortality_cumulative_per_million: string, hosp_patients: string, hosp_patients_per_million: string, icu_patients: string, icu_patients_per_million: string, location: string, new_cases: string, new_cases_per_million: string, new_cases_smoothed: string, new_cases_smoothed_per_million: string, new_deaths: string, new_deaths_per_million: string, new_deaths_smoothed: string, new_deaths_smoothed_per_million: string, new_people_vaccinated_smoothed: string, new_people_vaccinated_smoothed_per_hundred: string, new_tests: string, new_tests_per_thousand: string, new_tests_smoothed: string, new_tests_smoothed_per_thousand: string, new_vaccinations: string, new_vaccinations_smoothed: string, new_vaccinations_smoothed_per_million: string, people_fully_vaccinated: string, people_fully_vaccinated_per_hundred: string, people_vaccinated: string, people_vaccinated_per_hundred: string, positive_rate: string, reproduction_rate: string, stringency_index: string, tests_per_case: string, tests_units: string, total_boosters: string, total_boosters_per_hundred: string, total_cases: string, total_cases_per_million: string, total_deaths: string, total_deaths_per_million: string, total_tests: string, total_tests_per_thousand: string, total_vaccinations: string, total_vaccinations_per_hundred: string, weekly_hosp_admissions: string, weekly_hosp_admissions_per_million: string, weekly_icu_admissions: string, weekly_icu_admissions_per_million: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(covid_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
